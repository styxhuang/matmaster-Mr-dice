"""
ç®€å•çš„ LLM æµ‹è¯•è„šæœ¬ - ç›´æ¥æµ‹è¯• API è¿æ¥
"""
import json
import os
import sys
from pathlib import Path

import requests
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
project_root = Path(__file__).parent.parent
env_path = project_root / ".env"
load_dotenv(env_path)


def get_llm_config():
    """è·å– LLM é…ç½®"""
    provider = os.getenv("LLM_PROVIDER", "deepseek")
    model = os.getenv("LLM_MODEL", "deepseek/deepseek-chat")
    api_base = os.getenv("LLM_API_BASE", "").strip() or None
    api_key = os.getenv("LLM_API_KEY", "").strip() or None
    
    # è§£æ API baseï¼ˆä¸åšå®¹é”™ï¼Œå¿…é¡»æ˜¾å¼é…ç½®ï¼‰
    if not api_base:
        raise ValueError("LLM_API_BASE is not set")
    api_base = api_base.rstrip("/")
    
    return {
        "provider": provider,
        "model": model,
        "api_base": api_base,
        "api_key": api_key,
    }


def test_llm_config():
    """æµ‹è¯• LLM é…ç½®"""
    print("=" * 60)
    print("æµ‹è¯• LLM é…ç½®")
    print("=" * 60)
    
    try:
        config = get_llm_config()
        print(f"Provider: {config['provider']}")
        print(f"Model: {config['model']}")
        print(f"API Base: {config['api_base']}")
        print(f"API Key: {'å·²è®¾ç½® âœ…' if config['api_key'] else 'âŒ æœªè®¾ç½®'}")
        
        if not config['api_key']:
            print("\nâš ï¸  è­¦å‘Š: LLM_API_KEY æœªè®¾ç½®")
            print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: LLM_API_KEY=your_api_key_here")
            return False, None
        
        return True, config
    except Exception as e:
        print(f"âŒ é…ç½®é”™è¯¯: {e}")
        return False, None


def test_llm_connection(config):
    """æµ‹è¯• LLM è¿æ¥"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• LLM è¿æ¥")
    print("=" * 60)
    
    try:
        url = f"{config['api_base']}/chat/completions"
        headers = {
            "Authorization": f"Bearer {config['api_key']}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": config["model"],
            "messages": [
                {"role": "system", "content": "You are a helpful assistant. Reply briefly."},
                {"role": "user", "content": "Say 'Hello, LLM is working!' if you can read this."},
            ],
            "temperature": 0.2,
        }
        
        print(f"å‘é€è¯·æ±‚åˆ°: {url}")
        print(f"Model: {config['model']}")
        print("ç­‰å¾…å“åº”...")
        
        response = requests.post(url, headers=headers, json=payload, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        content = data["choices"][0]["message"]["content"]
        
        print(f"\nâœ… LLM å“åº”æˆåŠŸ!")
        print(f"å“åº”å†…å®¹: {content}")
        return True
    except requests.exceptions.RequestException as e:
        print(f"\nâŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        if hasattr(e, 'response') and e.response is not None:
            try:
                error_data = e.response.json()
                print(f"é”™è¯¯è¯¦æƒ…: {error_data}")
            except:
                print(f"å“åº”çŠ¶æ€ç : {e.response.status_code}")
                print(f"å“åº”å†…å®¹: {e.response.text[:200]}")
        return False
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_preprocessing_simple(config):
    """æµ‹è¯•é¢„å¤„ç†åŠŸèƒ½ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•é¢„å¤„ç†åŠŸèƒ½ï¼ˆæ„å›¾è¯†åˆ«ï¼‰")
    print("=" * 60)
    
    try:
        url = f"{config['api_base']}/chat/completions"
        headers = {
            "Authorization": f"Bearer {config['api_key']}",
            "Content-Type": "application/json",
        }
        
        system_prompt = "You are a material database search assistant. Return strict JSON only."
        user_prompt = """Input Query: æ‰¾ä¸€äº› Fe2O3 ææ–™

Return JSON:
{
  "material_type": "crystal|mof|unknown",
  "domain": "semiconductor|catalyst|battery|perovskite|zeolite|other",
  "confidence": 0.0-1.0
}"""
        
        payload = {
            "model": config["model"],
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "temperature": 0.2,
        }
        
        print(f"æŸ¥è¯¢: æ‰¾ä¸€äº› Fe2O3 ææ–™")
        print("æ­£åœ¨ä½¿ç”¨ LLM è¿›è¡Œæ„å›¾è¯†åˆ«...")
        
        response = requests.post(url, headers=headers, json=payload, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        content = data["choices"][0]["message"]["content"]
        print('content: ', content)
        # å°è¯•è§£æ JSON
        import re
        json_match = re.search(r'\{[\s\S]*\}', content)
        if json_match:
            result = json.loads(json_match.group(0))
            print(f"\nâœ… é¢„å¤„ç†æˆåŠŸ!")
            print(f"ææ–™ç±»å‹: {result.get('material_type', 'N/A')}")
            print(f"é¢†åŸŸ: {result.get('domain', 'N/A')}")
            print(f"ç½®ä¿¡åº¦: {result.get('confidence', 'N/A')}")
            return True
        else:
            print(f"\nâš ï¸  å“åº”æ ¼å¼å¼‚å¸¸: {content[:100]}")
            return False
            
    except Exception as e:
        print(f"\nâŒ é¢„å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 60)
    print("MrDice LLM æ¥å…¥æµ‹è¯•")
    print("=" * 60)
    
    results = []
    
    # æµ‹è¯• 1: é…ç½®æ£€æŸ¥
    print("\n[1/3] æ£€æŸ¥é…ç½®...")
    config_ok, config = test_llm_config()
    results.append(("é…ç½®æ£€æŸ¥", config_ok))
    
    # å¦‚æœé…ç½®æœ‰é—®é¢˜ï¼Œç›´æ¥è¿”å›
    if not config_ok:
        print("\n" + "=" * 60)
        print("âŒ é…ç½®æ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆé…ç½® LLM_API_KEY")
        print("=" * 60)
        print("\nè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®:")
        print("  LLM_PROVIDER=deepseek")
        print("  LLM_MODEL=deepseek/deepseek-chat")
        print("  LLM_API_KEY=your_api_key_here")
        return
    
    # æµ‹è¯• 2: LLM è¿æ¥
    print("\n[2/3] æµ‹è¯• LLM è¿æ¥...")
    results.append(("LLM è¿æ¥", test_llm_connection(config)))
    
    # æµ‹è¯• 3: é¢„å¤„ç†
    if results[-1][1]:  # å¦‚æœè¿æ¥æˆåŠŸ
        print("\n[3/3] æµ‹è¯•é¢„å¤„ç†åŠŸèƒ½...")
        results.append(("é¢„å¤„ç†åŠŸèƒ½", test_preprocessing_simple(config)))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ±‡æ€»")
    print("=" * 60)
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
    
    all_passed = all(r[1] for r in results)
    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLM å·²æˆåŠŸæ¥å…¥ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚")
    print("=" * 60)


if __name__ == "__main__":
    main()

